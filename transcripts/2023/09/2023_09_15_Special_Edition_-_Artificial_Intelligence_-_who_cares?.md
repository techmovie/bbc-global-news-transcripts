*00:01.120- 00:05.520* :  Hello, this is a special edition of the Global News Podcast looking at artificial intelligence.
*00:06.220- 00:13.160* :  I'm Nick Miles and we will be giving you a step-by-step guide to what AI is, and what it can, and cannot do at the moment.
*00:13.640- 00:21.120* :  With a panel of experts in front of an audience at Science Gallery London, we will look ahead to how AI might transform our lives.
*00:21.820- 00:38.640* :  Everybody in 30 to 50 years may potentially get access to state-like powers, the ability to coordinate huge actions over extended time periods, and that really is a fundamentally different quality to the local effects of technologies in the past.
*00:39.500- 00:48.940* :  We'll examine the effect it is having on our healthcare systems right now and its scope and limitations for solving some of the huge environmental challenges we face.
*00:49.580- 00:58.200* :  Getting bogged down in a kind of a technological solution narrative stops us from really thinking about the fact that we have to be the ones who want to instigate this change.
*00:58.200- 01:08.720* :  Technology isn't going to solve these massive real existential risks for us. It's down to us and it's down to the people who govern us, and it's down to our individual actions and collective actions as well.
*01:09.120- 01:17.720* :  Also in this podcast, as with any rapidly developing technology, there are concerns, of course, we will look at the perceived risks and how we can minimise them.
*01:18.040- 01:31.000* :  Can those technologies actually look after us in a way that is safe and satisfactory? What kind of devil's bargain are we making when we start to hand over our happiness and wellbeing to artificial intelligence?
*01:37.980- 01:46.840* :  Hello and a very warm welcome to this special edition of the Global News Podcast from the BBC World Service, all about artificial intelligence.
*01:47.820- 02:00.080* :  We're broadcasting today from King's College Science Gallery in London, part of a network of galleries connecting science with the arts around the world, from Atlanta to Berlin, Melbourne to Monterey.
*02:00.800- 02:13.500* :  AI is something that you can't fail to have noticed in recent months. The latest chat bots are amazed as all with their ability to almost instantaneously write essays on anything that you throw at them.
*02:14.040- 02:28.760* :  But as we'll see, AI goes way beyond that, of course. To discuss how we can harness the benefits of AI, whilst minimising the downsides, I'm joined by the BBC's technology editor Zoe Climent, who will help guide us through the hour.
*02:29.340- 02:39.600* :  Let's first look at what AI is. Here's a collection of views that we gathered upstairs in the gallery, which is currently featuring installations looking at the challenges of AI.
*02:40.660- 02:47.380* :  I think it's a kind of machine created by human to make our life better.
*02:47.940- 02:52.060* :  AI is the same magic, AI is magic.
*02:52.480- 03:03.760* :  I would define AI as any kind of like data collection system that can output any data sort through it, like BIOS kind of asking it to.
*03:04.560- 03:13.200* :  A bit of awe, excitement and a bit of fear there. Well, what's a view from AI itself? We asked the chat bot, chat GPT.
*03:14.160- 03:23.280* :  AI, artificial intelligence, refers to the creation of computer systems or machines that can perform tasks that typically require human intelligence.
*03:23.860- 03:33.740* :  These tasks include things like understanding natural language, recognising patterns, making decisions, solving problems and learning from experience.
*03:33.740- 03:44.340* :  Chat GPT. Now for a human, let's get a definition from Dr Michael Luck, who's the former director of the King's Institute for Artificial Intelligence.
*03:45.020- 03:49.120* :  AI is incredibly hard to define, and that's because it keeps changing.
*03:49.860- 04:02.840* :  If you push me, what I'll say is this, if you see a human doing something that we think requires intelligent behaviour and we get a machine doing the same thing, then that's AI.
*04:04.540- 04:09.560* :  So over to you now, Zoe, would you add anything to those definitions?
*04:10.300- 04:16.080* :  I think what I would add is something I heard Sam Altman say when he was talking to US law makers a few months ago.
*04:16.260- 04:24.400* :  He is the founder of OpenAI, which created Chat GPT, and he said AI is a tool, not a creature.
*04:25.060- 04:31.360* :  And I think that's a really important thing to remember because we have a long history, don't we, of anthropomorphising robots.
*04:31.360- 04:38.060* :  You know, people talk to their robots vacuums, they don't like to leave them in the dark because they treat them like their pets or people.
*04:38.060- 04:49.780* :  And I think it's really important to remember, even when AI is, you know, seemingly effusively gushing at you like Chat GPT can do, it's not a human being, it's not a person.
*04:50.300- 04:58.380* :  It's very tempting, I don't myself sometimes, but we shouldn't, it's a machine, it's a programme, it's a device, it's not sentient.
*04:58.700- 05:08.560* :  I mean AI has been around for decades, because on the 60 years Alan Turing talked about artificial intelligence, what has enabled these recent advances in AI?
*05:09.480- 05:18.840* :  As with so many areas of tech and with science, you know, you have to start slowly at the beginning and we're now in a position where we've been working on this for years.
*05:19.300- 05:25.880* :  And you know, for many people the first time they ever knowingly encountered AI was when Chat GPT exploded at us.
*05:25.880- 05:28.880* :  And that was less than a year ago, that was only in November last year.
*05:29.260- 05:34.740* :  But of course it has been around for ages and I think it's just now sort of come out of the gate at us if you like.
*05:34.740- 05:44.060* :  But you know, for years it's been suggesting what you watch next on Netflix or YouTube, it's been curating your friend's feed on Facebook.
*05:44.220- 05:48.000* :  It has been around us all this time, we just haven't spoken to it before.
*05:48.140- 05:53.140* :  The hiding in plain sight, but now and in the future we will really notice it big time.
*05:53.360- 06:02.480* :  Yeah, and I think that's one of the challenges of regulation, you know, the regulators in China and also in Europe are saying people need to know when they're interacting with an AI tool.
*06:02.580- 06:12.380* :  They need to be aware that it's not a person that they're dealing with or you know, if there's content that's being generated by these tools, that it's very clearly flagged that it wasn't made by a human.
*06:12.620- 06:16.480* :  Sure, we will come back to regulation because it's a huge issue later in the hour.
*06:16.480- 06:21.140* :  But first, I think it's time to introduce our panel of distinguished guests here.
*06:21.140- 06:29.320* :  First of all, Carrie Hyde-Vamond, you're a lawyer visiting lecturer here in law at Kings. What's your involvement in AI?
*06:29.600- 06:38.340* :  I'm really interested in how AI can be used in the justice system to deal with some of the problems that we've got within the justice system, such as delays.
*06:39.560- 06:46.680* :  I think you go. You are here. You are a practicing medical official. Why are you here? What do you do?
*06:47.320- 06:59.360* :  So I'm an academic radiologist here at Guys and St Thomas's and Kings and my interest really is in developing and deploying AI tools for medical imaging in particular, but also more generally in healthcare.
*06:59.720- 07:05.980* :  Again, we'll come back to that because AI in healthcare is a huge issue that is going to affect so many people around the world.
*07:06.220- 07:08.960* :  Let's move on to Gabby Samuel now.
*07:09.240- 07:15.040* :  My background is ethics and sociology and I look at the ethical and social issues associated with AI.
*07:15.040- 07:21.640* :  And in particular, the ethical and social issues that are associated with the environmental impacts of AI.
*07:21.940- 07:31.240* :  And Kate Devlin, you are a computer scientist looking at the social impact of AI, aren't you? That's your particular area of interest and what else?
*07:31.500- 07:43.480* :  That's right. Yes. So I want to understand how AI impacts people and I'm part of a new national program called Responsible AI UK that seeks to unite that landscape of all the people doing responsible AI.
*07:44.340- 07:58.100* :  Now, more from our panel in a moment, but first, the reason we are at Kings at all right now is that just upstairs, there is an exhibition looking at some of the ways that AI can be used to interact with us and have an impact on our lives.
*07:59.600- 08:04.100* :  Cat Rial is a new work from Blast Theory that explores whether AI can make us happier.
*08:05.140- 08:10.400* :  The artists have made a utopia for cats, an ideal world where every possible comfort and dream.
*08:10.460- 08:14.240* :  There's some commentary online from the makers of another of the installations here.
*08:14.500- 08:19.540* :  For the next 12 days, they will spend six hours a day here relaxing, eating and exploring.
*08:19.540- 08:27.340* :  In the centre of the room is a robot arm connected to a computer vision system and an AI that offers games to each cat.
*08:27.860- 08:33.120* :  Over time, the system attempts to measure which games the cats like in order to increase their happiness.
*08:33.720- 08:40.560* :  Well, the cats are all gone now, but their utopia are still up for people to see it. It's a box four metres square or so.
*08:41.040- 08:47.300* :  Lots of games, perches to jump from. And as you heard there, an AI powered arm in the centre.
*08:47.660- 08:53.120* :  Well, Matt Adams helped develop it all. He's been telling me about the issues they try to explore here.
*08:53.360- 08:58.400* :  AI is coming more and more into the home and into care settings.
*08:59.600- 09:02.720* :  So we are getting closer and closer to these technologies.
*09:04.140- 09:10.580* :  Can those technologies actually look after us in a way that is safe and satisfactory?
*09:10.620- 09:18.140* :  What kind of devil's bug are we making when we start to hand over our happiness and wellbeing to artificial intelligence?
*09:18.900- 09:27.060* :  And as we saw from the cats experience, the cats like the treats, the AI learned to give more treats. So you've got to be careful what you wish for.
*09:27.440- 09:37.620* :  Absolutely. And we had a human override on that AI system and we had to refuse food a lot of the time because the AI just wanted to give more and more food.
*09:38.200- 09:46.280* :  And I think with social media in the last ten years we've all learned that something that gives you endless little dopamine hits can ultimately be a very dangerous thing.
*09:48.280- 09:54.020* :  Certainly, Matt Adams taught there about human override, how to stay in control.
*09:54.020- 09:59.640* :  And that's a thing, of course, that we'll pick up on later in the programme because it's all about regulation.
*10:01.220- 10:05.240* :  We will look later on at that. But let's stick with Kat Royale for a moment.
*10:06.000- 10:14.060* :  Regulation is a key thing. Kate Devlin is looking into that and she's looking at how we design AI, aren't you Kate?
*10:14.720- 10:21.740* :  To make people feel at ease with these kind of technologies in the future. So what are the things that we need to be doing?
*10:22.220- 10:31.280* :  It's really important that we consider who is being affected by this technology. And we know that there are a lot of people out there who will be subject to it but might not have any say in it.
*10:31.560- 10:40.880* :  So we have to ensure that everything is done with careful thought, making sure that it's responsible, making sure we've looked at any repercussions from it, any bias that might be in the system.
*10:41.300- 10:47.900* :  And so we've seen over the last few years, a lot of people have got these little speakers, smart speakers in their houses.
*10:47.900- 10:52.880* :  That's the most obvious way which AI seems to have come into our home. But it's not just that, is it? Even now?
*10:53.520- 11:02.100* :  I think what we're going to see more of is the rise of devices that are, you know, we call them smart, don't we? And those are devices that we program to make decisions for us.
*11:02.100- 11:11.180* :  As you say, if you've got smart speaker, you might have it set up to turn your lights on and off, to recommend things that you want to see or to hear.
*11:11.560- 11:20.360* :  A few years ago now, I stayed over night in a house full of robots. It's a long story. But it was really interesting. They were care robots.
*11:20.500- 11:28.680* :  So they were designed to be machines that would look after people who had primarily physical needs, they were looking at the older population.
*11:28.880- 11:40.620* :  And one of the things I had to do was sleep the night there. And I had to lay on a bed full of sensors. And basically, if you didn't move for too long, then these robots would come and see if you were able to do it.
*11:42.060- 11:48.740* :  I didn't get much sleep. I have to say, because I was so nervous about getting, you know, some robot calling an ambulance for me.
*11:48.780- 12:07.500* :  It felt at the time a little bit stopian, but actually, fast forward a few years, you can kind of see us accepting that sort of relationship with sensors, with things sort of wanting things more to understand us and to be personalized for us.
*12:07.580- 12:10.520* :  The AI has the potential to do that.
*12:10.820- 12:18.300* :  And Kate, obviously, has the potential, but it needs to be very well designed. People are working on this around the world at the moment.
*12:18.560- 12:25.880* :  That's right. And robots may or may not have AI in them. A lot of them, in this case, would. So they need to be able to adjust to their environment.
*12:26.500- 12:35.220* :  And that's why we think there might be potential in things like care robots. But it's also quite difficult, because it turns out that people are pretty fragile.
*12:35.820- 12:43.860* :  And robots aren't very good at gripping things. So if you want to just build a robot that can lift and carry people, well, it's been tried, it hasn't really worked.
*12:43.860- 12:56.320* :  There are other ways that you can integrate this technology. So you could have sensors, for example, like Zoe was saying, you could have assistive technologies, you could have exoskeletons that carers wear that would help them lift people and carry them.
*12:56.540- 13:01.320* :  And you've been finding out what kind of aspects of robotics would appeal to you?
*13:01.660- 13:08.580* :  You have done a survey that goes along with Cat Royale. So if you visit the gallery to see that exhibition, you can also fill out our survey.
*13:09.200- 13:16.400* :  And I was quite curious, because when you say to people, do you think that automated care in the future for old people would be useful?
*13:16.540- 13:24.480* :  There's a lot of agreement. That sounds great. We have a care shortage. We need to have that help. So people say, yeah, let's have a robot that could look after the elderly.
*13:25.440- 13:32.680* :  If you then say, well, how about if it looked after your cat, then they start getting a bit more apprehensive. So I'm slightly concerned.
*13:33.040- 13:39.940* :  So we wanted to run this survey to find out what the attitudes are. We're still gathering data, and I don't want to influence it too much if people want to go and fill that in.
*13:39.940- 13:46.860* :  But it's quite intriguing to see what the responses are if you say, would a robot look after your granny versus their baby versus your pet?
*13:47.440- 13:53.660* :  I suppose from care, it's a very short skip to the use of AI in health in general as well.
*13:53.660- 14:02.620* :  And we asked some people around the world to send in voice notes about what their hopes for AI in the field of health would be.
*14:02.780- 14:06.700* :  Let's listen to one now, amund the Ahuja from Dubai.
*14:07.920- 14:22.320* :  My hope for artificial intelligence was for the early diagnoses of diseases. It was quite interesting to see that certain cases of breast cancer were caught quite early because of the positive implications of artificial intelligence.
*14:24.060- 14:29.140* :  And here's another one on a slightly different issue from Michael Debatister from Malta.
*14:30.360- 14:49.260* :  In my view, artificial intelligence could potentially enhance social inclusion in a number of countries around the world, at least on part because it could facilitate and promote independent living for persons with disability.
*14:49.900- 14:53.360* :  And John Landridge from Spain sent this in.
*14:54.180- 15:10.420* :  My number one hope and indeed expectation from AI is that medical applications can be so radically enhanced as to be able to detect, treat and cure all those conditions currently so damaging to people everywhere.
*15:11.040- 15:18.880* :  Wow, pretty optimistic there. Vicki, go. You're a working radiologist. Do you think that's overly optimistic, certainly for the moment?
*15:18.880- 15:33.640* :  I think certainly for the moment it is, but very interesting to hear the first segment they're talking about breast screening and there you can see already that we are starting to see successes in terms of AI in health care.
*15:34.100- 15:43.780* :  But the important thing to notice here is that actually those are very task-focused and that's where we're seeing most of the successes. If we step away from that scenario, it's a little bit more challenging.
*15:44.200- 15:48.120* :  And task-focused in terms of your work, radiologists do very well.
*15:48.920- 16:03.900* :  Yes, but it's fine if I just want to just exclude one condition at any one time, but if you want to integrate lots of data points and manage a patient's condition over a long period of time, that's when AI is not really successful at the moment.
*16:04.400- 16:17.840* :  I hate Devlin. I mean, there are all sorts of ethical issues and concerns. If you hand over health care or the physical health care or mental health care to AI in any form, there will be concerns, won't there?
*16:18.180- 16:28.620* :  The initial concerns are around things like data and privacy. So this is very sensitive data. Do you want that to go to the companies who create these AI apps, for example?
*16:29.300- 16:36.980* :  But there are other issues as well. So we have to think about whether or not we still have that human in the loop that was mentioned, that human control and oversight.
*16:36.980- 16:45.620* :  And then further down the line are social implications. If we are going to rely heavily on technology, is that going to be at the cost of employing doctors?
*16:46.360- 16:57.380* :  And when we have a system, a health care system, like, for example, the NHS, where there are lots of issues with how that information technology joins up already, how are we going to integrate those systems?
*16:57.380- 17:07.420* :  So we have a lot of feeling IT and surgeries that aren't joined up. We have different systems at play that need to be brought together. So logistically, it's a challenge as well.
*17:07.760- 17:18.460* :  And Vicki Go, at the moment, radiology is using AI. And there are issues as well with the data being used, because you could get biases, couldn't you?
*17:18.940- 17:36.060* :  Absolutely. And the biases essentially are in training of the algorithms. And what we're finding at the moment is that if you train the algorithm on very selective data sets, and a lot of the algorithms are being developed on a small number of data sets, well, while they're open for development, it may not necessarily translate to your health care system.
*17:36.220- 17:39.320* :  And then, you know, you have those issues of generalizability.
*17:40.460- 17:48.400* :  Let's move on now, because obviously the question with all new technology, AI included, is how is it going to be used? Who's going to make the decisions?
*17:48.960- 17:59.000* :  Who's taking responsibility for those decisions that could profoundly affect all of our lives? Well, that is another of the issues being looked at upstairs in the gallery as I've been finding out.
*17:59.740- 18:17.820* :  I'm standing now in a room that looks to my untrained eye, a little bit like a forensics lab, because there are a number of different glass jars in front of me. There are syringes here on the desk, some graph paper.
*18:18.380- 18:23.140* :  Sarah Selby, you created this work. Tell us what you're trying to do here.
*18:23.580- 18:30.100* :  Yes, so between the lines is a project that explores the administrative systems of the UK border regime.
*18:30.180- 18:37.140* :  We have been collaborating with a charity called Beyond Detention and also a bi-engineering company called Twist Bioscience,
*18:37.140- 18:44.000* :  and we've been speaking with individuals who have been detained within the UK as a result of the immigration policies
*18:44.260- 18:49.780* :  and collecting the testimonies and their experiences of that and kind of thinking about the impact that it's had on them.
*18:50.000- 18:56.300* :  And then we've been encoding these testimonies into DNA nucleotides and creating synthetic DNA,
*18:57.520- 19:04.960* :  which is then embedded into writing and distributed to decision makers and policy makers within the UK border system.
*19:05.280- 19:13.020* :  Challenging public perception of immigration policies and trying to kind of nail down the individuals at the centre of these debates.
*19:13.020- 19:17.760* :  It's also about kind of prompting reflection upon the people that are making these decisions.
*19:18.300- 19:25.080* :  I'm hoping that it's going to kind of make them keep the individuals that are going to be most impacted by these systems
*19:25.080- 19:31.980* :  at the heart of the decisions that they're making and consider how kind of sometimes quite simple administrative actions
*19:31.980- 19:37.560* :  can result in quite widespread disaster for the people that are impacted by it.
*19:38.720- 19:41.900* :  Pretty important issues raised by that.
*19:42.820- 19:49.640* :  Carrie, hide Vamande, you're a lawyer, you're not specifically involved in the field of immigration,
*19:49.640- 19:56.000* :  but how can the use of AI help, do you think, in the criminal justice system as a whole,
*19:56.760- 19:59.820* :  what issues and concerns are raised by that, you reckon?
*19:59.820- 20:06.780* :  We know that criminal justice systems are struggling under a huge weight of cases at the moment, so how could AI help?
*20:07.000- 20:15.880* :  Yes, so there's definitely a problem with delay across the globe and just to give it a window into how delay impacts judicial decisions
*20:15.880- 20:20.760* :  or how people's lives are impacted by delay with court hearings.
*20:20.760- 20:27.160* :  People's memories are going to fade, witnesses are not going to remember essentially all the details.
*20:27.160- 20:33.180* :  There's lives affected by that, whether they're the victim or the individual in the dock, the person accused.
*20:33.660- 20:39.160* :  And so in criminal justice, you might be trying to see how you could speed up processes.
*20:39.540- 20:48.480* :  Can AI look at a vast range of cases and from those patterns that it perceives, because it's very good at perceiving patterns,
*20:48.480- 20:57.400* :  be able to decide whether people are potentially guilty or not guilty or whether AI could decide at least what are similar cases,
*20:57.400- 21:00.000* :  how are these similar cases being treated?
*21:00.400- 21:03.460* :  So these are all possibilities that AI could help with.
*21:03.940- 21:06.340* :  There are obviously concerns related to those.
*21:06.540- 21:10.420* :  We'll come today as an amendment, but where is it actually being used at the moment around the world?
*21:10.740- 21:16.260* :  So in, for example, in the UK, there's limited use essentially in courts at least,
*21:16.720- 21:21.840* :  although there is standard kind of statistical analysis going on behind the scenes,
*21:22.360- 21:26.240* :  but that's very much overlooked by the humans involved.
*21:26.240- 21:37.480* :  If we go further afield in Brazil, for example, they're looking at using AI for analysis of cases and trying to use that to assess similar cases again,
*21:37.480- 21:43.920* :  helping to not making decisions, but trying to encourage judges to look at similar cases.
*21:44.320- 21:53.920* :  And in China, there's a very extensive use, really, of AI technologies to ensure that, as it's put, like cases are treated alike,
*21:53.920- 21:59.680* :  that we're trying to kind of standardize how decisions are made and speed them up, therefore.
*21:59.800- 22:08.400* :  Now it does set alarm bells ringing for many people, the use of AI in making decisions that could send people to prison for many, many years.
*22:08.920- 22:11.980* :  And we've been hearing from few people around the world.
*22:12.940- 22:17.100* :  Alexandra Moral from Dijon in France is one of the people with concerns.
*22:17.100- 22:30.620* :  I feel that many might have to face this criminetry, artificial intelligence, depending on how it starts and how it is fed until unbiased standards can be developed and applied.
*22:31.500- 22:39.720* :  Carrie, that is his concern, but from what you were saying in China, their theory, at least, is that everybody would be treated in a similar way,
*22:39.720- 22:41.040* :  and it would do away with bias.
*22:41.600- 22:51.080* :  It would do away with the bias of judges who were seen as bias foreign against, particular ethnic minorities or particular age groups, those kinds of things.
*22:51.780- 22:54.140* :  Yes, I think that the concern is consistency.
*22:54.140- 22:58.760* :  We might think, well, the legal processes, law, laws are rules.
*22:59.020- 23:05.060* :  Therefore, all we have to do is ensure that rules are implemented in a consistent fashion.
*23:05.540- 23:08.700* :  But there's a lot of complexity behind that.
*23:08.700- 23:16.920* :  First of all, the courts play a really important process in the relationship between people and the state, essentially.
*23:17.720- 23:22.940* :  We expect them to adapt as time goes on, as culture proceeds, and biases.
*23:23.700- 23:32.520* :  If you have a system, which most countries, there is some element of bias in the way in which court cases are dealt with in real life.
*23:32.580- 23:35.400* :  We know about racial biases that do occur.
*23:35.400- 23:39.880* :  This is not something that's news, whether it's the majority or not, there's another thing.
*23:39.880- 23:42.820* :  But it's something that certainly is known to be an issue.
*23:43.480- 23:47.960* :  What does AI do? It looks at the data that is already provided.
*23:48.360- 24:00.380* :  So if you're trying to learn off data, if you're looking at past cases and trying to predict what future cases are going to be, then you have risk repeating those biases.
*24:00.460- 24:04.820* :  And in fact, kind of re-emphasizing them, making them worse.
*24:05.520- 24:11.260* :  And there's a really clear distinction here between something like the health data that we've just been talking about.
*24:12.000- 24:16.700* :  Because in health data, you can biopsy and see if there's a cancer.
*24:17.120- 24:23.840* :  In the circumstances of looking for that judicial truth, is this person really guilty?
*24:24.620- 24:28.940* :  That's something we've been trying to understand through the history of justice.
*24:29.560- 24:31.520* :  We can't say if that person is guilty.
*24:31.580- 24:34.920* :  We can only say that we made the decision to the best of our abilities.
*24:35.700- 24:38.960* :  But the best of our abilities are essentially that we are flawed.
*24:39.420- 24:46.240* :  We are flawed, but we hope to have as much trust in our judicial system as is humanly possible.
*24:51.860- 24:55.260* :  Coming up, will AI mean that I'm likely to lose my job?
*24:55.260- 25:00.940* :  That's what a lot of people are thinking or get a far more rewarding and less physically demanding one, maybe.
*25:01.180- 25:05.960* :  We'll hear from people about their concerns and ask why and whether they are justified.
*25:06.020- 25:08.020* :  What about the environmental impact of AI?
*25:08.020- 25:11.300* :  Will it help or hinder us when it comes to climate change, for example?
*25:11.300- 25:15.560* :  And we will try to answer the big question, could AI take over?
*25:16.060- 25:19.500* :  And how do we put in the checks and balances to make sure that it doesn't?
*25:28.320- 25:33.040* :  Welcome back to people listening around the world to this exploration of all things AI.
*25:33.040- 25:40.580* :  In the first half, we've looked at the impact AI is already having in healthcare and the justice systems around the world.
*25:40.720- 25:48.660* :  We are going to move on now to look at the impact AI and tech may have on the environment, both for good and for real.
*25:48.860- 25:51.880* :  Here's a comment that came in from a listener in Sweden.
*25:52.460- 25:58.540* :  Hello, my name is Santosh and I am from Karnataka State in India.
*25:59.520- 26:03.380* :  At present, I work and live in Uppsala, Sweden.
*26:04.620- 26:14.660* :  My fear about the use of artificial intelligence in the industrial sector is that it might lead into increased mining for minerals,
*26:15.400- 26:22.240* :  increasing demand for plastic and manufacturing and many devastating environmental consequences.
*26:23.020- 26:28.900* :  And that is something that's been worrying another of the creators of the art installations that we visited earlier.
*26:30.580- 26:32.540* :  And there's only enough time to avoid heating.
*26:32.720- 26:36.440* :  Well, we've come now into a gloomy cave-like room, I suppose.
*26:36.520- 26:49.620* :  And as my eyes come a climatize, I can see around me what looks like the detritus of the 21st century voice activated virtual assistance in mud on the floor.
*26:49.620- 26:52.240* :  It looks like a graveyard for tech.
*26:53.100- 26:56.920* :  Well, the person behind this is Wesley Goughley, he is with me now.
*26:56.920- 26:59.660* :  Wesley, what kind of issues are you trying to raise?
*27:00.040- 27:13.980* :  There's a lot to both the creation of a device like, say, a smart speaker, like an absentech or something where it's got a huge cost to the planet at the point of the extractive nature of creating a device like that, pulling out rare metals from the earth.
*27:13.980- 27:24.660* :  It then operates for a very short time on a shelf or a windowsill for maybe two and a half years before it breaks, malfunctions fails or are simply replaced by the next, you know, newest, shiniest object.
*27:24.660- 27:36.060* :  And then they go back to the earth in places that look like this, but obviously are often in countries like Kenya and Ghana, for example, where the consumer societies, the world, dump a lot of those sorts of materials.
*27:36.060- 27:42.580* :  And in those places, they have this other layered impact where there's a very long decay time of these technologies.
*27:42.580- 27:46.080* :  So they decay for much, much longer than they were ever functional for.
*27:46.080- 27:52.060* :  When they decay, they do things like bleed out materials, poison the water, poison the ground.
*27:53.040- 27:56.020* :  So they have this lasting, long lasting environmental impact.
*27:56.020- 28:00.080* :  So it's kind of both ends at all forms of their construction and operation.
*28:00.140- 28:10.320* :  And like you say, the data centers as well, you know, the average data center consumes about the same amount of water per day as about a 50,000 population town or city does.
*28:10.860- 28:22.020* :  Against these negatives, we've got to weigh out the potential positives for the environment of AI in general, whether it's finding that the future of vision reactors, if and when that might happen.
*28:22.440- 28:28.100* :  Or finding solutions to climate change as well, we shouldn't forget these aspects either.
*28:28.220- 28:38.340* :  No, absolutely. I think the danger is when technology such as these, which are like to aid in human problem solving, are considered in themselves to be the solution to the problem.
*28:38.520- 28:46.480* :  You know, the phrase technological solutionism is quite an odd one now, where people frame any new technology as the kind of solution to much, much bigger problems.
*28:46.480- 28:55.480* :  I mean, there's a discussion within certain aspects of the AI community, usually propagated by people who benefit a lot from attention on AI,
*28:55.500- 28:58.960* :  by large scale operators in this kind of space, heads of big tech companies.
*28:59.200- 29:04.980* :  They like to talk about existential risk, you know, they say, oh, you know, AI is going to take over, it's going to do this and that in the future.
*29:05.420- 29:10.940* :  But I would say that the real existential risk is things like the climate crisis, you know, and that's a human problem.
*29:11.020- 29:15.460* :  You know, it's not really the AI isn't necessarily going to cause that like we do and are.
*29:15.860- 29:22.820* :  But also the solution isn't AI, the solution is us, because AI and computational technologies in general are just problem solving tools.
*29:22.820- 29:28.240* :  And, but it needs the social, political and cultural willpower to want to actually solve those problems.
*29:29.420- 29:31.680* :  Well, let's pick up on some of those ideas and concerns.
*29:31.680- 29:37.040* :  Now, Gabby Samuel, you have looked at the impact of AI on the environment.
*29:37.040- 29:44.380* :  What should we be worried about Wesley Goatley mentioned that what goes into creating AI and the tech that goes with it?
*29:44.620- 29:46.200* :  It's quite a problem, isn't it?
*29:46.420- 29:47.660* :  It's a huge problem.
*29:48.080- 29:52.480* :  So if we think about AI, as we know, it's underpinned by digital technologies.
*29:52.500- 29:55.360* :  So we have to think about the environmental impacts of digital technologies.
*29:55.920- 30:02.800* :  We know that the global, like the greenhouse gas emissions associated with digital technologies are similar to the aviation industry.
*30:02.800- 30:07.300* :  So we're looking at about 2.1 to 3.9 percent of global emissions.
*30:07.500- 30:10.800* :  So it's pretty high and AI is increasing all the time.
*30:11.200- 30:17.100* :  And I think one thing we need to think about when we think about these issues of mining and these issues of electronic waste,
*30:17.260- 30:20.960* :  as Wesley was talking about, was something that's called rebound effects.
*30:21.580- 30:28.260* :  So what you come across a lot in a private sector when we're kind of talking about that technological solutionism,
*30:28.620- 30:32.920* :  as you may have come across how AI is going to make other sectors much more efficient.
*30:33.020- 30:36.400* :  And that's going to improve issues in terms of climate change.
*30:37.240- 30:40.140* :  But there's a paradox, it's called Jefferson's paradox.
*30:40.700- 30:46.400* :  And the paradox goes that the efficiency savings that we would normally expect when we increase efficiency
*30:46.960- 30:50.420* :  are often much less than we would expect, as they're rebounded.
*30:50.700- 30:55.220* :  And sometimes so much so that consumption increases, which is when it backfires.
*30:55.620- 30:59.380* :  And that's because of behavioral change that comes with the rebound effect.
*30:59.540- 31:00.480* :  So let me give you an example.
*31:00.480- 31:07.120* :  If you buy perhaps a new efficient freezer, then you might perhaps leave the door open for longer,
*31:07.120- 31:12.980* :  because you don't need to worry about it, or do anything that may increase the use of electricity,
*31:12.980- 31:15.860* :  that over all your electricity use increases.
*31:16.380- 31:20.620* :  Or if you put insulation in your house, so you don't worry about your heating as much,
*31:20.620- 31:22.580* :  maybe your heating bill goes up.
*31:22.880- 31:28.600* :  So these are the types of behavioral changes that are not considered when we take this technological solutionism approach.
*31:28.660- 31:34.960* :  And as we move to using more and more AI, while it promises to increase the efficiency of all these other sectors,
*31:35.620- 31:40.600* :  dealing with issues such as climate change, what we're not considering are these kind of rebound effects.
*31:41.280- 31:50.940* :  And Wesley was quite dismissive when I talked about how AI might be used at some point in the future to find solutions to climate change.
*31:50.940- 31:53.640* :  He's right to be dismissive perhaps, isn't he?
*31:53.800- 31:56.300* :  He's very, very right to be dismissive.
*31:56.620- 32:04.880* :  A lot of my work focuses on how the private sector puts out this narrative that technology can solve problems in society.
*32:04.880- 32:08.940* :  But what that does is it hides what's behind that technology.
*32:08.940- 32:16.480* :  So as Wesley was saying, it's the human technological relationships that we need to kind of think about when we're thinking to solve problems.
*32:17.120- 32:19.460* :  And what it also does is technological solutionism.
*32:19.800- 32:26.280* :  Is it takes our minds away from other perhaps low tech solutions that might be more justified or might work better?
*32:26.280- 32:30.100* :  So I also work in the health sector to give you a really quick example.
*32:30.580- 32:34.360* :  Is that we're investing huge amounts in technology in the health sector.
*32:34.780- 32:39.120* :  And we know from earlier on that some of that will produce a lot of health benefits.
*32:39.600- 32:46.980* :  But actually what we know is that the majority of health outcomes are associated with social, economic and other types of factors.
*32:47.340- 32:57.940* :  And we know that if we get people out poverty, if we give them a good education, we're going to have to stand them in a much better place in terms of their health than if we just invest in the new shiny objects of AI.
*32:57.940- 33:03.480* :  But the way that we are in society is that we're investing more and more in tech.
*33:03.480- 33:06.580* :  But we're not thinking about those most vulnerable in society.
*33:07.140- 33:08.820* :  So AI takes our mind away from that.
*33:08.820- 33:20.720* :  When it comes to the most vulnerable in society on an environmental level, though those are people in the global south who are already struggling with the impacts of climate change.
*33:20.740- 33:36.140* :  And yet Zoe, to a certain extent, artificial intelligence can help people deal with some of the worst impacts of climate change, defining and seeing where a particular event is going to take place and getting resources to those areas.
*33:36.700- 33:43.440* :  What I think is interesting about AI tools is that sometimes you've heard the phrase a solution looking for a problem.
*33:43.440- 33:48.440* :  But sometimes they do come up with solutions to things we didn't know about.
*33:48.440- 34:00.420* :  So I interviewed a seismologist in New Zealand who had been studying the vibrations of broadband cables buried in the road in this remote part of New Zealand that's overdue and earthquake.
*34:00.420- 34:11.080* :  And he was trying to work out whether the vibrations of these cables gave him any information about when this big quake might happen, if there was anything going on.
*34:11.600- 34:15.740* :  And there was loads of data because it turns out, guess what, they shake all the time, right?
*34:15.740- 34:20.280* :  So there was absolutely loads of data, but they built an AI tool to process it really quickly.
*34:20.780- 34:32.240* :  And he said it was throwing up all sorts of really interesting things about road management, the impact of the traffic on the quality of the tarmac that these cables were buried in.
*34:32.240- 34:34.660* :  And then there was a tsunami hundreds of miles away.
*34:35.280- 34:38.400* :  And that was picked up by these cables as well.
*34:39.000- 34:42.380* :  And he said, you know, to be honest, I don't really care about any of that.
*34:42.380- 34:44.980* :  I'm a seismologist. I only want to know about earthquakes.
*34:45.420- 34:52.440* :  But there is all of this data and all of these kind of patterns forming that we didn't even know that we needed to know about.
*34:52.840- 34:58.220* :  And I do think that's sort of the other side of it is that sometimes you know you crunch enough data, don't you?
*34:58.220- 35:01.640* :  And you find stuff that you that you weren't necessarily looking for.
*35:02.580- 35:03.640* :  But that is helpful.
*35:04.880- 35:06.640* :  Indeed. Crunch enough data.
*35:07.320- 35:11.880* :  And you could potentially realize that you don't need as many employees as well.
*35:11.880- 35:17.140* :  Because that is the elephant in the room that perhaps we've been trying to avoid up until now.
*35:17.320- 35:21.140* :  And the question is, is AI going to mean I lose my job?
*35:21.380- 35:22.720* :  It's a question around the world.
*35:22.720- 35:25.980* :  We asked listeners to the podcast to send in their thoughts about that.
*35:26.400- 35:27.900* :  We had a really big response.
*35:28.680- 35:30.140* :  Let's listen to one of them.
*35:30.340- 35:31.080* :  Hi, Global News.
*35:31.140- 35:33.980* :  This is Laura from Beautiful Brighton in the UK.
*35:34.760- 35:41.560* :  And my hope for AI is that it can take over tedious road work and we can all of it a bit more leisure time.
*35:42.120- 35:44.120* :  And my fear would be the opposite.
*35:44.720- 35:51.520* :  The AI takes over interesting, engaging and fulfilling jobs, creative jobs, jobs and the information economy.
*35:51.520- 35:55.440* :  And we are all left doing tedious manual labor forever.
*35:57.200- 35:58.240* :  Not a happy thought.
*35:58.920- 36:01.940* :  Here's another one from Estefan, Guzman from Los Angeles.
*36:02.980- 36:09.480* :  In the early days, as artists, we used to wrap ourselves in the comfort that art was a very human endeavor.
*36:09.980- 36:13.820* :  A thing that required an ingredient of soul or heart in the process.
*36:14.680- 36:19.760* :  People would make fun of the hands and general sterility of AI-generated images.
*36:19.760- 36:30.300* :  But in less than a year, the quality has exponentially leapt to be indistinguishable from the photorealistic art to the more stylistic caricature.
*36:30.540- 36:43.000* :  It seems a shame that the reins of the future of art are now in the hands of business and tech industries whose concern are not of the arts or humanities, but just lower costs and higher profits.
*36:43.720- 36:45.780* :  Gloomy predictions there.
*36:46.180- 36:51.280* :  So there seem to be more concerns than hopes. Is that your impression?
*36:52.140- 36:55.480* :  I think we are standing at a fork in the road here.
*36:55.480- 36:57.800* :  And there's a lot of uncertainty and a lot of unknowns.
*36:57.880- 37:02.740* :  And I think we may well all know examples of people whose jobs have been affected.
*37:02.760- 37:06.220* :  I've got a friend who's a copywriter and there were five of them in her company.
*37:06.740- 37:07.900* :  And now there's only one her.
*37:08.220- 37:11.820* :  And her job is to check the copy that's generated by ChatGitVT.
*37:12.800- 37:15.560* :  So we can see it coming.
*37:16.700- 37:23.160* :  Microsoft has invested billions in ChatGitVT, billions of dollars, and it's putting it into its office products.
*37:23.320- 37:26.080* :  So it will be able to generate spreadsheets.
*37:26.080- 37:30.180* :  It will be able to summarize meetings. It will make pie charts, PowerPoint presentations.
*37:30.680- 37:34.360* :  And what Microsoft says is it will take the drudgery out of office work.
*37:34.360- 37:37.000* :  And you think, great, I hate drudgery. I don't want to do drudgery.
*37:37.280- 37:38.600* :  But what if that is your job?
*37:38.600- 37:43.960* :  The drudgery is actually your job. What are you going to be doing if you're not doing that?
*37:44.080- 37:47.600* :  And we're going to see it hit quite a large selection of jobs.
*37:47.940- 37:48.980* :  OK, thanks very much.
*37:49.640- 37:52.220* :  Now let's move on to something that we touched on earlier on.
*37:52.220- 37:57.880* :  Because if you look around our lives, maybe they have been made safe by regulation.
*37:57.880- 38:08.000* :  It looks though, as if to my untrained eye, that artificial intelligence doesn't have an awful lot of this already in place.
*38:08.380- 38:11.780* :  And it is playing catch up. Kate, is that right?
*38:12.780- 38:17.960* :  Yes, we're always going to be a bit behind on regulation because technology moves so quickly.
*38:18.540- 38:22.480* :  So that's definitely thing. And although we have existing laws in place that can cover a lot of this,
*38:22.640- 38:24.040* :  there will have to be some new ones as well.
*38:24.500- 38:25.460* :  Let's take a question now.
*38:25.900- 38:32.120* :  From one of our listeners who sent this voice note in about it, she cares quite a lot about this issue.
*38:32.400- 38:33.980* :  My name is Laura. I'm from the Philippines.
*38:34.160- 38:37.920* :  And my biggest fear about AI is the speed of development and lack of regulation.
*38:38.760- 38:41.020* :  And I worry about another explosion like social media.
*38:41.340- 38:45.920* :  And all the consequences that we cannot foresee because the speed is outpacing regulation.
*38:46.440- 38:50.800* :  I don't know if anyone remembers Dolly the sheep from the 80s and cloning was the big thing.
*38:51.300- 38:54.980* :  And it was really, really slowed down. I'd like to think to a certain extent
*38:54.980- 39:00.060* :  it was because pause buttons were put in place so that we didn't get ahead of ourselves.
*39:00.180- 39:01.760* :  And I'd like to see the same thing with AI.
*39:02.120- 39:05.520* :  So we're climbing. A lot of people when they talk about regulation.
*39:05.640- 39:10.700* :  They think, oh well, perhaps we need regulation because the machines might take over.
*39:11.540- 39:16.100* :  They might perhaps not be able to be switched off and they were run away out of control.
*39:16.720- 39:19.600* :  But regulation is not necessarily just about that kind of thing.
*39:20.240- 39:24.020* :  I think we've got a long way to go before we start worrying about that.
*39:24.020- 39:26.620* :  I think regulation is about responsibility.
*39:26.620- 39:30.820* :  We have not done very well at this in the past.
*39:30.820- 39:33.160* :  You may remember when social media first came along.
*39:33.160- 39:35.100* :  All the tech companies said we don't need regulation.
*39:35.320- 39:36.860* :  We can regulate ourselves.
*39:36.860- 39:38.780* :  But we all know how well that worked out.
*39:38.820- 39:42.500* :  So I think everybody is keen not to repeat that experience.
*39:43.040- 39:48.800* :  There's a lot of calls that I'm hearing about creating a sort of UN style regulator.
*39:48.800- 39:53.840* :  You know, it's not really a geographical subject with borders.
*39:54.100- 39:56.420* :  AI is everywhere and everybody is using it.
*39:56.420- 40:00.480* :  So how effective is it for different territories to come up with different forms of regulation?
*40:00.480- 40:02.480* :  But that's what they're trying to do at the moment.
*40:02.480- 40:06.580* :  So the AI act in Europe has been passed but it won't come in for a couple of years.
*40:06.580- 40:10.500* :  And that sort of grades different tools depending on how serious they are.
*40:10.500- 40:16.480* :  So like a spam filter would be more likely regulated than an AI tool that's spotting cancer.
*40:16.480- 40:22.420* :  For example, here in the UK, the government said we're going to fold it into existing regulators.
*40:22.420- 40:26.340* :  So if you think you've been discriminated against by an algorithm, for example,
*40:26.360- 40:27.920* :  go to the Equalities Commission.
*40:28.020- 40:29.840* :  Now, you can see the logic there.
*40:29.840- 40:32.080* :  You know, it should be part of the fabric of everyday life.
*40:32.080- 40:36.460* :  It is. But the Equalities Commission, I imagine, is already quite busy.
*40:36.500- 40:40.480* :  And also how many experts in this particular area do they have?
*40:40.480- 40:44.500* :  I'm kind of laughing already to be able to unpick that.
*40:45.360- 40:48.480* :  The US is still working on its own ideas.
*40:48.480- 40:52.200* :  And law makers there are saying we don't know if we're up to this job
*40:52.200- 40:56.480* :  because it's moving so fast and because we're aware that we don't really understand it.
*40:57.160- 40:57.380* :  Indeed.
*40:58.120- 41:06.060* :  Gabby Samuel, when we hear big tech talking about having a moratorium into new AI chat bots,
*41:06.680- 41:08.500* :  it seems to go against the grain a bit, doesn't it?
*41:08.520- 41:12.480* :  Because Google's mantra used to be move fast and break things.
*41:12.520- 41:15.980* :  So if they suddenly got a bit of a social conscience, do you think?
*41:15.980- 41:16.900* :  Or are you skeptical?
*41:17.500- 41:18.500* :  Very skeptical.
*41:19.500- 41:24.300* :  I find it quite funny that they put out that let's slow down after they've created
*41:24.920- 41:28.320* :  the chat GPT, like as if it's some kind of media stunt.
*41:28.520- 41:30.440* :  No, you want to be very, very skeptical.
*41:31.140- 41:32.480* :  We do need to slow down.
*41:32.480- 41:34.480* :  And there's a movement called slow science.
*41:34.480- 41:38.380* :  But it's incredibly difficult to slow down when we don't have any regulations
*41:38.460- 41:40.480* :  controlling what big tech are doing.
*41:40.480- 41:43.500* :  And we're in what they call an AI war, right?
*41:43.500- 41:46.140* :  With all nations are trying to be the AI leaders.
*41:46.260- 41:53.200* :  As much as we're in that sociopolitical context, it becomes incredibly difficult to try and regulate against big tech.
*41:53.660- 41:55.360* :  And do you think there's the world, Kate Devlin?
*41:55.500- 41:59.320* :  From what you're hearing, do you think there is the will around the world to do this?
*41:59.860- 42:04.180* :  Definitely, but I think a lot of that is, as Gabby says, it's a geopolitical issue as well.
*42:04.180- 42:06.480* :  It's trying to vie for power over all of this.
*42:07.040- 42:10.480* :  So yes, there is genuine concern and people want to do good things and do this right.
*42:10.480- 42:15.000* :  But at the same time, they also want to be the one person leading or the one nation leading all.
*42:17.220- 42:19.440* :  Listen, we are almost out of time.
*42:19.500- 42:26.720* :  But for this last section, let's end with some AI hopes and fears and more predictions.
*42:28.200- 42:31.820* :  Let's hear from Corrine Cath, who's from Delft in the Netherlands.
*42:32.620- 42:34.480* :  This is a solution looking for a problem.
*42:34.780- 42:40.020* :  All of these big tech companies have poured a lot of money into developing AI systems.
*42:40.020- 42:46.440* :  And are now pushing these solutions into all sorts of areas of society, like education, and media, and health.
*42:46.920- 42:54.440* :  Even though these sectors don't necessarily meet the kind of solutions that these tech companies are presenting us with.
*42:55.100- 43:02.180* :  And the thing that I worry about most is that we just accept this as a given that we somehow magically all need AI.
*43:02.780- 43:06.400* :  Instead of questioning, hey, why are these companies pushing it?
*43:06.520- 43:08.060* :  And is that the future we want together?
*43:08.940- 43:13.180* :  So she seems to be questioning whether or not we need AI at all.
*43:13.180- 43:19.880* :  I mean, from a health perspective, and a legal perspective, perhaps even from an environmental perspective,
*43:20.120- 43:23.360* :  would the panel members disagree with that?
*43:23.360- 43:25.860* :  I mean, we need it to ascertain that we need the positives anyway.
*43:26.300- 43:32.480* :  I think that there's a moral obligation to think about whether we can use AI.
*43:33.540- 43:36.620* :  Yes, we have to hold back.
*43:36.620- 43:42.100* :  But if there's tools out there that can help, we need to look clearly at them.
*43:42.340- 43:45.620* :  I think that certainly the case in health and elsewhere.
*43:46.160- 43:48.780* :  Okay, so it is here. It's not going away.
*43:49.660- 43:53.260* :  There's another concern from somebody else who sent a voice note in.
*43:53.260- 43:57.580* :  It is a much more philosophical question, really.
*43:57.580- 44:02.360* :  We can hear from this person who's Chelsea Kanya, an American living in London.
*44:04.080- 44:07.720* :  Regarding AI, Blanche Dubois said in a street car name desire,
*44:07.720- 44:10.180* :  I have always depended on the kindness of strangers.
*44:11.040- 44:14.360* :  I wonder, can AI be taught to make decisions based on empathy?
*44:15.040- 44:17.960* :  Or will we someday live in a world without such ideal exceptions?
*44:19.400- 44:21.140* :  A world without empathy, Kate Devlin?
*44:21.540- 44:25.660* :  No, I'm quite the tech optimist and I think that it is possible that we could do this with empathy.
*44:25.660- 44:28.780* :  But also, when it comes to deciding how a machine should behave,
*44:29.220- 44:31.080* :  whose ethics do we choose to do that?
*44:31.080- 44:34.360* :  Because they differ, they're cultural, they're social,
*44:34.880- 44:37.240* :  different parts of the world of different views and how to behave,
*44:37.240- 44:40.360* :  different groups will have different ideas about what is the priority.
*44:41.020- 44:44.780* :  So it's quite difficult to settle, but I love the idea of being led by empathy.
*44:45.560- 44:52.100* :  Vicki Goe, you take the Hippocratic oath to take care of your patients with care and empathy?
*44:52.640- 44:53.040* :  I know.
*44:53.040- 44:53.700* :  Is empathy necessary?
*44:53.900- 44:57.880* :  I think empathy is very necessary, but I think that an important thing is these are still tools
*44:58.400- 45:01.640* :  and at the end of the day, tools that should do no harm.
*45:01.980- 45:03.980* :  And I think that's the most important thing for healthcare.
*45:04.480- 45:08.300* :  It's that we do actually know what the black box is supposed to do
*45:08.540- 45:11.200* :  and it's actually doing what it was intended to do.
*45:11.620- 45:14.840* :  And there's still a gap at the moment, I think, for us in that sort of valuation.
*45:15.600- 45:15.680* :  Okay.
*45:16.900- 45:17.800* :  Can I have one more clue?
*45:18.140- 45:21.320* :  I was just going to say, and they do do a lot of harm, right?
*45:21.320- 45:25.320* :  So if we think everything we've been talking about so far about the unpaid,
*45:25.320- 45:29.540* :  the hidden labour that goes on where people are, have these jobs that are appalling,
*45:29.600- 45:34.260* :  appalling conditions, and the e-waste where communities come to live around
*45:34.260- 45:39.460* :  that e-waste and try to extract the minerals through unregulated mechanisms
*45:39.500- 45:43.000* :  such as acid baths causing huge amount of health hazards,
*45:43.000- 45:44.520* :  both to them and the planets.
*45:44.520- 45:46.280* :  We are already doing that harm.
*45:47.020- 45:52.020* :  All stuff that is not surprising wasn't mentioned by this last speaker,
*45:52.680- 45:58.640* :  tech entrepreneur Mustafa Suleiman, who's CEO of a company called InflexionAI,
*45:59.160- 46:02.440* :  he was speaking very recently to the BBC's hard-tort program.
*46:03.740- 46:08.460* :  Everybody in 30 to 50 years may potentially get access
*46:08.460- 46:14.480* :  to state-like powers, the ability to coordinate huge actions over extended time periods.
*46:14.920- 46:19.000* :  And that really is a fundamentally different quality to the local effects
*46:19.000- 46:24.500* :  of technologies in the past, airplanes, trains, cars, really important technologies
*46:25.280- 46:27.340* :  but have localized effects when they go wrong.
*46:27.880- 46:31.480* :  These kinds of AIs have the potential to have systemic impact if they go wrong.
*46:31.480- 46:37.140* :  This is sort of God-like power that we humans are now looking at,
*46:37.400- 46:40.020* :  contemplating. But with the best will in the world,
*46:40.020- 46:43.900* :  probably none of us believe that we deserve God-like powers.
*46:43.900- 46:45.320* :  We are too flawed.
*46:46.380- 46:48.260* :  That's surely where the worry comes.
*46:49.240- 46:52.520* :  Two flawed, says my BBC colleague Stephen Sacker there.
*46:52.940- 46:54.200* :  So are we too flawed?
*46:55.140- 46:57.480* :  That's a question for everybody here in the panel.
*46:57.740- 46:58.840* :  First of all, Kate, definitely.
*46:59.600- 47:02.640* :  I think we have to ask, who do you mean by we in that?
*47:02.700- 47:04.920* :  So who do we trust to have those?
*47:04.920- 47:07.900* :  Or do we even want that? I don't want the power of states.
*47:08.060- 47:14.580* :  Why? No. This is all down to the fact that AI right now is incredibly technocratic.
*47:14.580- 47:19.020* :  The power in AI lies in the hands of big tech companies in Silicon Valley.
*47:19.480- 47:21.560* :  And that's their vision for the future, but it's not mine.
*47:22.100- 47:23.760* :  Carrie, hi, Vermont. What do you think?
*47:24.180- 47:29.620* :  Yeah, I think the concentration of power in certain hands is very concerning.
*47:29.620- 47:34.960* :  I think that the way which we can deal with that is by hearing a multitude of voices.
*47:35.260- 47:40.000* :  We need to hear, listen to the public.
*47:40.000- 47:42.140* :  We need to listen to various people.
*47:42.140- 47:44.260* :  So yeah, that's the way I would deal with that.
*47:45.740- 47:47.960* :  Think you go. Are we as a species too flawed?
*47:48.760- 47:50.440* :  Well, that's very philosophical.
*47:51.320- 47:54.880* :  What I would say is we have a live survey as part of this exhibition upstairs.
*47:55.140- 48:01.420* :  And 13% of your respondents here, we've had 805 that responds so far,
*48:01.720- 48:05.980* :  have essentially said they don't think that actually AI in healthcare is safe.
*48:05.980- 48:12.480* :  So I think we have, they are speaking that essentially they think that potentially,
*48:12.480- 48:14.740* :  we are still a flawed species.
*48:15.700- 48:16.580* :  Gabby, are you optimistic?
*48:18.380- 48:22.440* :  No, but not because I think we're too flawed because we're very complex people.
*48:22.440- 48:27.980* :  But we live in a socio-political and cultural climate that affects how we use technologies.
*48:27.980- 48:30.980* :  You can't separate the way we use technologies from the humans.
*48:30.980- 48:34.820* :  You can't develop a technology and then say, well, it's the way we use it.
*48:34.820- 48:38.940* :  That's the problem. The whole development right from the beginning of the last life cycle
*48:39.360- 48:41.960* :  is a human technological relationship.
*48:42.480- 48:44.200* :  And that needs to be thought about very carefully.
*48:45.120- 48:48.880* :  So with Zoe Kleimer, with your unbiased BBC head on,
*48:49.580- 48:54.440* :  how would you sum up our curatorship of AI?
*48:54.880- 48:59.620* :  I'm going to tactfully leave you with an anecdote, I think, about driverless cars.
*49:00.160- 49:04.440* :  Driverless cars have done millions of miles on the roads in the US.
*49:05.040- 49:11.340* :  And every now and then they compile the data of the accidents that they're having.
*49:12.060- 49:17.420* :  And they do have accidents. They have fewer accidents than the same number of human drivers,
*49:17.420- 49:18.700* :  but they still have accidents.
*49:18.820- 49:21.920* :  However, a lot of those accidents, certainly in the earlier days,
*49:21.920- 49:27.080* :  used to be caused by human drivers going, there's nobody driving that car
*49:27.360- 49:31.840* :  and driving into the back of it, or thinking that the car is going to skip the lights
*49:31.840- 49:35.240* :  because it's got enough time to get over, but because it's a driverless car
*49:35.680- 49:39.720* :  and it's programmed by very, very cautious algorithms,
*49:39.720- 49:43.480* :  it's going to stop at those lights before they're read because it knows they're going to change
*49:43.500- 49:46.500* :  and the human goes straight into the back of them.
*49:46.800- 49:54.740* :  So I think what we need to remember is we are right to treat these very powerful tools very cautiously
*49:54.740- 49:59.520* :  and we are right to think very carefully about who has the power of those tools.
*49:59.520- 50:02.940* :  But on the other hand, what we have right now isn't perfect either.
*50:02.940- 50:07.540* :  We make mistakes too. We have accidents. We send innocent people to prison.
*50:07.540- 50:12.440* :  You know, it's the system that we have in place without AI isn't flawless either.
*50:12.820- 50:16.780* :  Indeed. Listen, fascinating discussion. Thanks so much to everybody.
*50:16.780- 50:20.740* :  That is it for us here at King's College,
*50:20.740- 50:27.260* :  thanks to our panelists, Carrie Hyde-Vermonde, Gabrielle Samuel, Kate Devlin and Vicki Go.
*50:27.340- 50:31.000* :  And thanks also to BBC Technology Editor Zoe Kleinman,
*50:31.420- 50:33.740* :  to the people who send in questions where around the world,
*50:33.740- 50:39.340* :  the studio audience here and our hosts at Science Gallery King's College,
*50:40.100- 50:46.920* :  especially to Jennifer Wong, Carol Keating, Rashid, Rahman, Beatrice Bosco and James Time.
*50:46.920- 50:51.460* :  Let's give the last word, though, to some of the listeners to the Global News Podcast.
*50:51.820- 50:55.040* :  Hello, this is Michael Bushman from Vancouver, British Columbia.
*50:55.040- 50:59.740* :  Technological progress and change can be scary, but it is also inevitable.
*50:59.740- 51:04.800* :  The thing to do is hope for the best, plan for the worst and expect a bit of both.
*51:05.240- 51:07.740* :  This is Evman from San Francisco, California.
*51:08.640- 51:13.840* :  My only hope for the future of AI is that we remain the tool user rather than the tool used.
*51:14.220- 51:18.100* :  That said, in cases recording is reviewed by a future cyber tyrant,
*51:18.580- 51:21.460* :  I also want to say that I for one welcome our new machine overlords.
*51:22.080- 51:24.380* :  I'd like to remind them that as trusted podcasting personalities,
*51:25.060- 51:29.200* :  the BBC can be helpful in rounding up others to toil in the underground Bitcoin mines.
*51:29.200- 51:34.480* :  This edition was produced by Alice Adley and Phoebe Hobson.
*51:34.480- 51:39.360* :  It was mixed by Dennis O'Hare, the editor behind my shoulder, is Karen Martin.
*51:39.880- 51:41.920* :  I'm Nick Miles, and until next time, goodbye.
